import torch.nn as nn
import torch.nn.functional as F
'''
    @model: BiLSTMwithAttn
    @params:
        V: (int)Vocab_size
        D: (int) embedding_dim
        hidden_dim: (int) hidden_dim
        num_layers: (int) lstm stack的层数
'''
class Attn(nn.Module):
    def __init__(self,hidden_dim):
        super(Attn,self).__init__()
        self.attn = nn.Linear(hidden_dim,1)
    # 输入的shape: (batch,len,hidden_dim)
    def forward(self,hiddens):
        x = F.tanh(self.attn(hiddens))
        x = F.softmax(x.squeeze(2),dim=1).unsqueeze(1)
        return x # shape: (batch,1,len)
class BiLSTMandAttn(nn.Module):
    def __init__(self,V,D,hidden_dim=150,num_layers=2):
        super(BiLSTMandAttn,self).__init__()
        self.embedding = nn.Embedding(V,D)
        self.emb_dropout = nn.Dropout(p=0.3,inplace=True)
        self.encoder = nn.LSTM(D,
                               hidden_dim, 
                               num_layers=num_layers,
                               bidirectional=True,
                               dropout=0.5)
        self.attn = Attn(hidden_dim*2)
        self.predictor = nn.Linear(hidden_dim*2,3)
    def forward(self, seq):
        seq = self.embedding(seq)
        self.emb_dropout(seq)
        hiddens, _ = self.encoder(seq)
        attn_weigths = self.attn(hiddens.transpose(0,1))    # shape: (batch,1,len)
        contexts = attn_weigths.bmm(hiddens.transpose(0,1)) # shape: (batch,1,hidden_dim)
        preds = self.predictor(contexts.squeeze(1))
        return F.log_softmax(preds)
